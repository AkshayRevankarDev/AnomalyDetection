\documentclass[final]{article}

\usepackage{nips_2017}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{float}

\title{Anomaly Detection in healthcare diagnostics using Image synthesis}


\author{
  Sai Theja~Bajjuri, Akshay Mohan~Revankar  and Rohan~Marar\\
  Group \#: 19 \\
  University at Buffalo\\
  Buffalo, NY 142603 \\
  \texttt{\{sbajjuri;author2\}@buffalo.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Detecting and interpreting abnormalities in medical images remains challenging due to the wide variability in pathological appearance and the limited availability of labeled data. In this work, we propose a multi-modal framework for anomaly detection and counterfactual reconstruction in brain magnetic resonance imaging (MRI). Our approach leverages a Vector-Quantized Variational Autoencoder (VQ-VAE) trained exclusively on healthy brain scans to learn a discrete latent representation of normal anatomical structure. When presented with tumor-affected scans, deviations from the learned healthy distribution manifest as localized reconstruction errors, enabling unsupervised anomaly detection and spatial localization. To further enhance interpretability, the detected abnormal scans are optionally processed by a supervised convolutional classifier to identify tumor categories. Additionally, we employ a Transformer-based generative model that utilizes self-attention and contextual latent information to synthesize a plausible healthy reconstruction of the tumor-affected brain, providing a counterfactual representation of normal anatomy. Qualitative results demonstrate that the proposed framework effectively highlights anomalous regions and produces anatomically coherent reconstructions, illustrating the potential of combining generative modeling and attention mechanisms for interpretable medical image analysis.
\end{abstract}

{\section{Introduction}}\label{sec:intro}
Medical image analysis plays a critical role in the diagnosis and treatment of neurological disorders, particularly in the detection of brain tumors from magnetic resonance imaging (MRI). Traditional supervised learning approaches for tumor detection rely heavily on large, accurately labeled datasets, which are costly to obtain and often limited in clinical settings. Moreover, discriminative classifiers typically provide only categorical predictions, offering limited insight into the underlying structure of abnormalities or their deviation from normal anatomy.

An alternative paradigm treats tumor detection as an anomaly detection problem, where pathological regions are modeled as deviations from the distribution of healthy tissue. In this setting, models are trained exclusively on normal data to learn a representation of healthy anatomy, and abnormalities are identified when test samples cannot be adequately explained by this learned distribution. Reconstruction-based methods, particularly autoencoder variants, have been widely explored for this purpose, as reconstruction errors can serve as spatial indicators of anomalous regions.

Recent advances in generative modeling have further improved the expressiveness of such approaches. Vector-Quantized Variational Autoencoders (VQ-VAEs) introduce discrete latent representations that enable stable training and preserve fine-grained structural information, making them well-suited for modeling complex anatomical patterns. However, while reconstruction error can localize anomalies, it does not directly address the question of how the anatomy might appear in the absence of pathology.

To address this limitation, we propose a multimodal framework that combines unsupervised anomaly detection with generative counterfactual reconstruction. First, a VQ-VAE is trained on healthy brain MRI scans to learn a discrete latent representation of normal anatomy. Tumor-affected scans are then reconstructed using this model, and regions with elevated reconstruction error are interpreted as anomalies. Optionally, a supervised convolutional neural network can be applied to the detected anomalous regions to classify tumor types, providing complementary diagnostic information.

Beyond detection and classification, we employ a Transformer-based generative model to synthesize a plausible healthy version of a tumor-affected brain scan. By leveraging self-attention mechanisms and contextual information from the learned latent representations, the Transformer models global anatomical dependencies and produces a counterfactual reconstruction that approximates normal brain structure. This reconstruction is not intended as a clinical ground truth, but rather as an interpretable representation of how the anatomy may appear in the absence of pathology.

The primary contributions of this work are threefold: (i) an unsupervised anomaly detection framework based on VQ-VAE reconstruction error for localizing brain tumors, (ii) an optional supervised classification component for tumor identification, and (iii) a Transformer-based approach for counterfactual healthy brain reconstruction that enhances interpretability. Together, these components form an integrated pipeline for interpretable anomaly detection and reconstruction in medical imaging.\\

\section{Related Works}\label{sec:past}

Anomaly detection in medical imaging has been widely studied, particularly for brain tumor identification from MRI scans. Early approaches primarily relied on supervised convolutional neural networks (CNNs), which achieve strong performance when large labeled datasets are available \cite{havaei2017brain}. However, such methods often struggle with generalization to rare tumor types and provide limited interpretability regarding the nature of detected abnormalities.

To mitigate the dependence on labeled data, unsupervised and semi-supervised anomaly detection methods have gained increasing attention. Reconstruction-based models, including autoencoders and variational autoencoders (VAEs), model the distribution of healthy tissue and identify abnormalities through elevated reconstruction error \cite{schlegl2017unsupervised}. This paradigm has been successfully applied to brain MRI, where reconstruction error maps highlight lesion regions without requiring explicit tumor annotations \cite{baur2021autoencoders}.

Standard VAEs, however, often produce overly smooth reconstructions that limit precise anomaly localization. Vector-Quantized Variational Autoencoders (VQ-VAEs) address this limitation by learning discrete latent representations that preserve fine structural details \cite{van2017neural}. Discrete latent models have been shown to better capture anatomical patterns and improve unsupervised anomaly detection in medical imaging \cite{zimmerer2019unsupervised}. Our work builds on this line of research by training a VQ-VAE exclusively on healthy brain MRI scans and treating tumors as out-of-distribution regions.

Attention-based models, particularly Transformers, have recently been adopted for medical image analysis due to their ability to model long-range dependencies and global context \cite{dosovitskiy2020image}. While these models have demonstrated strong performance in supervised segmentation and classification tasks, their use in unsupervised or generative medical imaging remains limited.

In contrast to prior work, our approach combines reconstruction-based anomaly detection with generative counterfactual reconstruction. While existing methods focus primarily on localization or classification, few address the problem of synthesizing a plausible healthy version of a tumor-affected brain. Related ideas in counterfactual generation for medical imaging have been explored in limited settings \cite{pawlowski2020deep}, but remain underdeveloped for brain tumor analysis. By integrating VQ-VAE-based anomaly detection with a Transformer-driven generative model, our framework provides both localized anomaly identification and interpretable healthy reconstructions, distinguishing it from prior approaches.



\section{Data}
Also, as a part of this section, provide a detailed description of the nature of the data you worked on. What type of data is it? How or where did you get the data? How much will you have access to and will you be able to successfully evaluate your work with it (i.e.~the types of labels/annotations that exist on the dataset)? Did you have to do any preprocessing, filtering, or other special treatment to use this data in your project? If this is from another project or paper, include appropriate references. If you are downloading it, say from \emph{github}, add a link to the location of the data on-line and a brief description of what its original use.

\subsection{Visual data}
This is just a demonstration of incorporating multiple images within one structure to show how to use tabular columns within graphics environment.
\begin{figure}[H]
	%\vspace*{-7mm}
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.5\linewidth]{image1} &
		\includegraphics[width=0.5\linewidth]{image2} \\
		(a) Image 1: colored network &
		(b) Image 2: monochrome network \\
	\end{tabular}
	\caption{Two neural network diagrams colored differently}\label{img:2nns}
\end{figure}

\subsection{Audio data}
This is another subsection but here we introduce how to use tables such as in Table \ref{tab:table1} below. This is very common in the evaluation or results sections.
%
\begin{table}[H]
\centering
\caption{An example of a table reporting results.}\label{tab:table1}
	\begin{tabular}{|cccccc|}
		\hline
		Group & One     & Two     & Three    & Four     & Sum      \\ \hline\hline
		Red   & 1000.00 & 2000.00 &  3000.00 &  4000.00 & 10000.00 \\ \hline
		Green & 2000.00 & 3000.00 &  4000.00 &  5000.00 & 14000.00 \\ \hline
		Blue  & 3000.00 & 4000.00 &  5000.00 &  6000.00 & 18000.00 \\ \hline\hline
		Sum   & 6000.00 & 9000.00 & 12000.00 & 15000.00 & 42000.00 \\ \hline\hline
	\end{tabular}
\end{table}


\section{Methods}\label{sec:approach}
Discuss your approach for solving the problem that you set up in the introduction. Why is your approach the right thing to do? Did you consider alternative approaches? You should demonstrate that you have applied ideas and skills built up during the semester to tackling your problem of choice. It may be helpful to include figures, diagrams, or tables to describe your method or compare it with other methods.


\section{Experiments and Results}\label{sec:expts}
Discuss the experiments that you performed to demonstrate that your approach solves the problem. The exact experiments will vary depending on the project, but you might compare with previously published methods, perform an ablation study to determine the impact of various components of your system, experiment with different hyperparameters or architectural choices, use visualization techniques to gain insight into how your model works, discuss common failure modes of your model, etc. Describe any evaluation methods used here including different metrics that you use to confirm that your technique really does work well. Present any numbers, tables or graphs here, to illustrate your experimental results.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{learning}
	\caption{Sample figure showing the training and validation errors from a learning system.}
	\label{fig:aus}
\end{figure}

\section{Limitations and ethical implications}\label{sec:limits}
State the limitations of your approach and explain why things did not work as well or why they worked so well. Also, explain why your work might not generalize

\section{Conclusion and future work}\label{sec:concl}
Summarize your key results - what have you learned? What are your conclusions from the entire process; what did you leave undone and why? If someone else was going to pick up where you left off, what will be the most important aspects they should focus on? In general, suggest ideas for future extensions or new applications of your idea.  \medskip

\small
%Your references will be automatically populated here as long as you enter them correctly into your .bib file. An example is attached
\bibliographystyle{plainnat}
\bibliography{Report_template}

\end{document}











